# 🏛️ CNN 모델의 진화: AlexNet부터 ResNet까지

CNN의 기본 원리를 배웠으니, 이제 이 원리들이 어떻게 조립되어 이미지 인식의 역사를 뒤바꾼 전설적인 모델들을 탄생시켰는지 그 발전 과정을 살펴보겠습니다.

> 💡 **미리 알아두기**
>
> 지금부터 소개할 모델들은 구조가 다소 복잡하게 느껴질 수 있습니다. 모든 디테일을 암기하기보다는, **'더 좋은 성능을 위해 이런 아이디어와 방식으로 네트워크를 발전시켰구나'** 하는 큰 흐름을 이해하는 데 초점을 맞추시면 됩니다.

## 경쟁의 장, 이미지넷(ImageNet) 챌린지

### 정의

**이미지넷(ImageNet)** 은 1,400만 장 이상의 방대한 이미지 데이터베이스이며, **ILSVRC(ImageNet Large Scale Visual Recognition Challenge)** 는 이 이미지넷 데이터를 이용해 2010년부터 2017년까지 열린 세계적인 컴퓨터 비전 경진대회입니다.

### 설명

ILSVRC는 1,000개의 클래스(종류)를 가진 약 140만 장의 이미지를 누가 더 정확하게 분류하는지를 겨루는 대회였습니다. 이 대회는 CNN 모델들이 엎치락뒤치락하며 발전하는 치열한 기술 경쟁의 장이 되었고, 오늘날 우리가 배울 딥러닝의 역사에서 중요한 이정표들을 남겼습니다.

---

## 1. AlexNet (2012) - 게임의 판도를 바꾸다

### 정의

**AlexNet**은 2012년 ILSVRC에서 압도적인 성능(Top-5 에러율 15.3%)으로 우승하며, **딥러닝과 CNN의 시대가 왔음을 세상에 알린 기념비적인 모델**입니다. (논문 저자인 Alex Krizhevsky의 이름을 따 AlexNet으로 불립니다.)

### 설명

> **비유: 대회에 처음으로 '슈퍼컴퓨터'를 들고 온 팀 💻**
>
> AlexNet 이전의 모델들이 일반 CPU로 계산할 때, AlexNet은 **GPU의 병렬 연산 능력**을 본격적으로 활용하여 훨씬 더 크고 깊은 모델을 빠르게 훈련시켰습니다. 또한, 당시로서는 혁신적이었던 **ReLU 활성화 함수**와 **드롭아웃(Dropout)** 규제 기법을 도입하여 기울기 소실 문제를 해결하고 과적합을 방지했습니다. 이 '신기 술'들로 무장한 AlexNet의 등장은 딥러닝 연구의 폭발적인 증가를 이끄는 기폭제가 되었습니다.

---

## 2. VGGNet - 깊고 단순함의 미학

### 정의

**VGGNet**은 AlexNet 이후, 모델의 깊이가 성능에 미치는 영향을 탐구한 모델입니다. 이 모델의 핵심 특징은 복잡한 기교 없이, **오직 3x3 크기의 작은 컨볼루션 필터**만을 반복적으로 쌓아 네트워크를 매우 깊게(19개 층) 만들었다는 점입니다.

### 설명

> **비유: 최고의 재료 하나로 승부하는 장인 👨‍🍳**
>
> VGGNet은 '더 넓은 특징을 보려면 큰 필터가 필요하지 않을까?'라는 통념을 깼습니다. 5x5 크기의 필터 하나를 사용하는 것과, **3x3 크기의 필터를 두 번 연속으로 사용**하는 것이 결과적으로 동일한 영역을 보면서도(receptive field), 파라미터 수는 더 적고 비선형성을 더 많이 추가할 수 있어 효과적임을 증명했습니다. 즉, 하나의 단순하고 좋은 부품(3x3 필터)을 깊게 쌓는 것만으로도 최고의 성능을 낼 수 있다는 '깊이의 중요성'을 보여준 모델입니다.

---

## 3. GoogLeNet (2014) - 넓고 효율적인 혁신, 인셉션

### 정의

**GoogLeNet**은 2014년 ILSVRC 우승 모델로, VGGNet보다 훨씬 적은 파라미터(약 5백만 개)로 더 높은 성능을 달성한 **효율적인 네트워크**입니다. 이 모델의 핵심은 **인셉션 모듈(Inception Module)** 이라는 독창적인 블록 구조에 있습니다.

### 설명

> **비유: 만능 공구 상자, 스위스 아미 나이프 🇨🇭**
>
> 이전 모델들이 '이번 층에서는 3x3 필터를 쓰자'처럼 하나의 연산만 선택했다면, 인셉션 모듈은 **'1x1, 3x3, 5x5 컨볼루션과 풀링을 일단 전부 다 해보고, 그 결과들을 합쳐서 가장 좋은 특징을 알아서 학습하게 하자'** 는 아이디어를 구현했습니다. 또한, **1x1 컨볼루션**을 중간에 끼워 넣어 데이터의 깊이(채널 수)를 줄이는 '차원 축소' 기법을 통해, 계산량을 획기적으로 줄이면서도 성능은 높이는 두 마리 토끼를 모두 잡았습니다.

---

## 4. ResNet (2015) - 깊이의 한계를 넘어서다

### 정의

**ResNet(Residual Network)** 은 2015년 ILSVRC 우승 모델로, **'잔차 학습(Residual Learning)'** 이라는 개념과 **'스킵 연결(Skip Connection)'** 구조를 도입하여, 이전까지 불가능하다고 여겨졌던 **152층**이라는 압도적인 깊이의 네트워크를 성공적으로 학습시킨 모델입니다.

### 설명

> **비유: 막힘 없는 고속도로를 건설하다 🛣️**
>
> 네트워크가 너무 깊어지면 역전파 과정에서 기울기가 제대로 전달되지 못하는 '기울기 소실' 문제가 발생합니다. (마치 복잡한 시내 도로에서 신호가 막히는 것처럼)
>
> **스킵 연결**은 몇 개의 층을 건너뛰어 입력을 출력에 바로 더해주는 '고속도로' 같은 지름길입니다. 이 구조 덕분에 네트워크는 전체 출력을 처음부터 배우는 대신, **입력과 출력의 차이(잔차, Residual)** 만을 쉽게 학습하면 되므로, 기울기가 막힘없이 전달되어 수백, 수천 개의 층을 쌓는 것이 가능해졌습니다. ResNet은 이 구조를 통해 3.57%라는 에러율로 **인간의 이미지 인식 능력을 처음으로 뛰어넘는** 역사적인 성과를 기록했습니다.

---

## ✨ 핵심 요약

> **CNN 모델들은 ImageNet 챌린지라는 경쟁의 장에서 (AlexNet) GPU와 새로운 기법을 도입하고, (VGGNet) 깊이의 중요성을 탐구하며, (GoogLeNet) 더 효율적인 구조를 고안하고, 마침내 (ResNet) 깊이의 한계를 뛰어넘는 방식으로 진화해왔습니다. 이 모델들의 핵심 아이디어들은 현대 컴퓨터 비전 기술의 근간을 이루고 있습니다.**
