# 🧭 경사 하강법의 세 가지 길: BGD, SGD, 그리고 미니배치

이전에 '경사 하강법'을 이용해 모델이 손실(오차)을 줄이는 방향으로 학습한다는 것을 배웠습니다. 이는 마치 산의 가장 낮은 계곡을 향해 한 걸음씩 내려가는 과정과 같았죠.

그런데 이 '한 걸음'을 내딛기 전에 **"얼마나 많은 지형(데이터)을 살펴볼 것인가?"** 에 따라 경사 하강법은 크게 세 가지 전략으로 나뉩니다. 각 전략을 알아보기 전에, 먼저 핵심 용어인 **'배치'** 에 대해 확실히 알아보겠습니다.

---

## 🎯 기본 개념: 배치(Batch)란 무엇일까?

딥러닝 모델을 훈련시킬 때, 전체 데이터를 한 번에 처리하는 것은 비효율적입니다. 그래서 데이터를 작은 묶음으로 나누어 처리하는데, 이 **'데이터 묶음'** 하나하나를 **배치(Batch)** 라고 부릅니다.

> **비유: 문제집 풀기 📚**
>
> - **전체 데이터셋**: 풀어야 할 문제집 한 권 전체 (e.g., 2,000 문제)
> - **배치(Batch)**: 그중에서 한 번에 나눠서 풀기로 한 '문제 묶음'
> - **배치 크기(Batch Size)**: 한 묶음에 들어있는 '문제의 개수' (e.g., 100 문제)
> - **이터레이션/스텝(Iteration/Step)**: 한 묶음(배치)을 다 푸는 행위. 즉, **가중치를 한 번 업데이트**하는 과정입니다.
> - **에폭(Epoch)**: 문제집 **전체**를 처음부터 끝까지 한 번 다 푸는 것.

---

## 1. 배치 경사 하강법 (BGD): 가장 신중한 등반가

**배치 경사 하강법(Batch Gradient Descent)** 은 가장 신중하고 원칙적인 방법입니다.

> **전략 비유**: 산 전체의 지형도(**전체 훈련 데이터**)를 모두 분석한 후, 가장 확실한 내리막길을 딱 한 번 찾아 **한 걸음**만 내딛는 방식입니다.

- **동작 방식**: 전체 훈련 데이터셋을 모두 사용해 손실을 한 번 계산하고, 그 기울기를 바탕으로 딱 한 번만 가중치를 업데이트합니다. (1 에폭 = 1 업데이트)
- **장점**:
  - 전체 데이터를 고려하므로 학습 경로가 매우 안정적이고 예측 가능합니다.
- **단점**:
  - 데이터가 수백만 개라면, 단 한 걸음을 떼기 위해 수백만 개의 데이터를 모두 계산해야 합니다. 이는 엄청난 계산 비용과 메모리를 요구하며, 학습 속도가 매우 느립니다.

---

## 2. 확률적 경사 하강법 (SGD): 가장 빠른 탐험가

**확률적 경사 하강법(Stochastic Gradient Descent)** 은 BGD의 비효율성을 개선하기 위해 등장한 가장 과감한 방법입니다.

> **전략 비유**: 지도를 전혀 보지 않고, 오직 **자신의 발밑에 있는 땅(데이터 1개)** 의 경사만 보고 일단 한 걸음 빠르게 내딛고 보는 방식입니다.

- **동작 방식**: 훈련 데이터셋에서 **무작위로 데이터 샘플 1개**를 뽑아 손실을 계산하고, 즉시 가중치를 업데이트합니다. 이 과정을 데이터 개수만큼 반복합니다. (1 에폭 = 데이터 개수만큼의 업데이트)
- **장점**:
  - 계산이 매우 빨라 학습 속도가 빠릅니다.
  - 업데이트 경로가 매우 불안정하게 움직이는데(noise), 이 덕분에 얕은 지역 최솟값(local minimum)에 빠지더라도 쉽게 탈출하여 더 좋은 전역 최솟값(global minimum)을 찾을 가능성이 있습니다.
- **단점**:
  - 학습 경로가 너무 불안정해서, 최저점 근처에서도 계속 크게 흔들리며 수렴하는 데 시간이 오래 걸릴 수 있습니다.

---

## 3. 미니배치 경사 하강법: 가장 현실적인 전략가

**미니배치 경사 하강법(Mini-batch Gradient Descent)** 은 위 두 방법의 장점만을 취한 절충안으로, **현대 딥러닝에서 가장 널리 사용되는 표준**과 같은 방법입니다.

> **전략 비유**: 산 전체 지도를 보는 대신, **자신 주변의 일부 지형(데이터 묶음, 미니배치)** 만 빠르게 훑어보고 다음 걸음을 결정하는 영리한 방식입니다.

- **동작 방식**: 전체 데이터를 **'미니배치'** 라는 적절한 크기의 묶음으로 나눕니다. 그리고 각 미니배치 단위로 손실을 계산하고 가중치를 업데이트합니다.
  - 예: 데이터 1,000개, 배치 크기 100 → 총 10개의 미니배치 생성 → 1 에폭당 10번의 업데이트 발생
- **장점**:
  - **메모리 효율성 & 빠른 속도**: 전체 데이터를 한 번에 메모리에 올릴 필요가 없어 큰 데이터셋에 효과적이며, SGD만큼은 아니지만 충분히 빠릅니다.
  - **안정성 & 최적화**: BGD의 안정성과 SGD의 무작위성을 모두 가집니다. 학습이 안정적이면서도 지역 최솟값을 효과적으로 탈출할 수 있습니다.
- **주의할 점**:
  - **배치 크기**라는 새로운 하이퍼파라미터가 생깁니다. 배치 크기가 너무 작으면 SGD처럼 불안정해지고, 너무 크면 BGD처럼 비효율적이 될 수 있어 적절한 값을 찾는 것이 중요합니다.

---

## ✨ 핵심 요약

세 가지 경사 하강법은 '한 번 업데이트할 때 얼마나 많은 데이터를 사용할 것인가'에 따라 구분됩니다.

| 구분                  | **배치 경사 하강법 (BGD)** | **확률적 경사 하강법 (SGD)** | **미니배치 경사 하강법** |
| :-------------------- | :------------------------- | :--------------------------- | :----------------------- |
| **업데이트당 데이터** | **전체 데이터**            | **1개**                      | **미니배치 (N개)**       |
| **장점**              | 안정적인 경로              | 빠른 속도, 지역 최솟값 탈출  | **속도와 안정성의 균형** |
| **단점**              | 매우 느림, 메모리 비효율   | 불안정한 경로                | 배치 크기 설정 필요      |
| **주 사용처**         | 작은 데이터셋, 이론적 분석 | (거의 사용 안 함)            | **딥러닝 표준**          |

결론적으로, **미니배치 경사 하강법**은 현대 딥러닝이 빠르고 효율적으로 학습할 수 있도록 하는 핵심적인 최적화 전략이라고 할 수 있습니다.
