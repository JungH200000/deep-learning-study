# ⚙️ 똑똑한 훈련의 비밀: 정규화와 배치 정규화

딥러닝 모델을 안정적이고 빠르게 학습시키기 위한 필수 기술 중 하나인 **정규화(Normalization)** 에 대해 알아보겠습니다.

## 정규화(Normalization)란?

### 정의

**정규화**는 데이터의 값 범위를 일정하게 조정하거나 데이터의 분포를 표준화하는 과정을 의미합니다. 이는 모델의 학습 과정 전반에 걸쳐 적용될 수 있는 중요한 기법입니다.

### 비유와 설명

> **비유: 단위를 통일하는 저울 ⚖️**
>
> 어떤 재료는 `gram` 단위로, 어떤 재료는 `kilogram` 단위로 측정한다면 요리사가 레시피를 파악하기 어렵겠죠? 정규화는 이처럼 제각각인 데이터의 단위를 **'0과 1 사이'** 와 같은 공통된 스케일로 통일시켜, 모델이 각 데이터의 중요도를 동등하게 보고 더 쉽게 학습할 수 있도록 돕는 과정입니다.

정규화는 입력 데이터의 특성 범위를 맞추는 **데이터 전처리** 단계뿐만 아니라, 신경망 내부의 각 층에서 신호가 전달될 때 그 분포를 일정하게 유지하는 **배치 정규화** 기법으로도 활용됩니다.

---

## 배치 정규화 (Batch Normalization)

### 정의

**배치 정규화**는 신경망의 각 층에서 활성화 함수로 들어가는 입력 값의 분포를 **미니배치 단위로 재조정**하는 기법입니다. 이는 신경망 안에 '자동 정규화 레이어'를 추가하는 것과 같습니다.

### 왜 필요할까? - 내부 공변량 변화 (Internal Covariate Shift)

신경망은 여러 층을 거치며 학습합니다. 이때 앞쪽 층의 가중치가 업데이트되면, 뒤쪽 층이 받는 데이터의 분포가 계속해서 달라지는 현상이 발생하는데, 이를 **내부 공변량 변화**라고 합니다.

> **비유: 움직이는 과녁 맞히기 🎯**
>
> 뒤쪽 층의 입장에서 보면, 자신이 맞춰야 할 과녁(데이터 분포)이 매번 미세하게 움직이는 것과 같습니다. 이렇게 되면 학습이 불안정해지고 속도가 느려질 수밖에 없습니다.
>
> **배치 정규화**는 각 층마다 과녁을 중앙에 다시 고정시켜주는 역할을 하여, 모든 층이 안정적으로 학습에만 집중할 수 있도록 돕습니다.

### 수식

배치 정규화는 다음과 같은 수식으로 계산됩니다.

$$
\text{BN}(X) = \gamma \frac{X - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}} + \beta
$$

- **$\text{BN}(X)$**: 배치 정규화를 거친 **최종 출력값**
- **$X$**: 현재 미니배치의 **입력 데이터**
- **$\mu_{\text{batch}}$**: 현재 미니배치의 **평균**
- **$\sigma_{\text{batch}}^2$**: 현재 미니배치의 **분산**
- **$\gamma$ (감마)**: 정규화된 데이터의 **크기를 조절**하는 학습 가능한 파라미터 (Scale)
- **$\beta$ (베타)**: 정규화된 데이터의 **위치를 조절**하는 학습 가능한 파라미터 (Shift)
- **$\epsilon$ (엡실론)**: 분모가 0이 되는 것을 방지하기 위한 아주 작은 값

이 과정은 크게 두 단계로 이루어집니다.

1.  **표준화**: 미니배치의 평균과 분산을 이용해 데이터를 평균 0, 분산 1의 분포로 만듭니다. $\left( \frac{X - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}} \right)$
2.  **크기 및 위치 조절**: 모델이 학습을 통해 스스로 최적의 분포를 찾을 수 있도록, 학습 가능한 파라미터 $\gamma$와 $\beta$를 이용해 데이터의 스케일과 위치를 미세 조정합니다.

### 배치 정규화의 효과

배치 정규화를 사용하면 다음과 같은 강력한 이점을 얻을 수 있습니다.

- **학습 속도 향상**: 학습률을 더 높게 설정할 수 있어 훈련 속도가 빨라집니다.
- **학습 안정성 증가**: 내부 공변량 변화를 줄여 학습 과정을 안정시킵니다.
- **기울기 문제 완화**: 기울기 소실/폭주 문제를 완화하여 더 깊은 네트워크를 훈련할 수 있게 돕습니다.
- **과적합 방지**: 일종의 규제(Regularization) 효과가 있어 과적합을 방지하는 데 도움을 줍니다.

---

## ✨ 핵심 요약

> **정규화는 데이터의 스케일을 맞추어 모델 학습을 돕는 중요한 기법입니다. 특히, 배치 정규화는 신경망 내부에 '자동 정규화 장치'를 달아 각 층이 받는 데이터 분포를 안정시켜, 더 빠르고 안정적인 학습을 가능하게 하는 핵심 기술입니다.**

- **정규화**: 데이터의 단위를 통일하여 학습 효율을 높이는 과정.
- **배치 정규화**: 신경망 내부 각 층의 입력 분포를 미니배치 단위로 정규화하는 기술.
- **내부 공변량 변화**: 배치 정규화가 해결하고자 하는 핵심 문제로, 층마다 입력 분포가 달라지는 현상.
- **주요 효과**: 학습 속도 및 안정성 향상, 기울기 문제 완화, 과적합 방지.
