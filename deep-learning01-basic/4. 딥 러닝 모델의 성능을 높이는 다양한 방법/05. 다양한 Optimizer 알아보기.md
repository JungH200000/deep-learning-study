# 🚀 더 빠르고 똑똑하게: 딥러닝 옵티마이저(Optimizer) 완전 정복

안녕하세요! 우리는 경사 하강법을 통해 모델의 손실(오차)을 줄여나간다는 것을 배웠습니다. 하지만 손실 함수라는 울퉁불퉁한 산맥에서 가장 낮은 계곡을 찾아가는 길은 결코 순탄치 않습니다.

이번 시간에는 이 험난한 여정을 더 빠르고, 안정적이며, 똑똑하게 헤쳐나갈 수 있도록 도와주는 다양한 최적화 전략, 즉 옵티마이저에 대해 알아보겠습니다.

## 옵티마이저(Optimizer)란?

**옵티마이저(최적화 알고리즘)** 는 신경망에서 손실을 최소화하기 위해 **가중치를 어떤 방식으로 업데이트할지 결정하는 방법**을 의미합니다. 각 알고리즘의 원리를 알아두면 나중에 더 복잡한 문제를 해결하는 데 큰 도움이 됩니다.

> 💡 **미리 알아두기**
>
> PyTorch, TensorFlow와 같은 딥러닝 프레임워크를 사용하면, 복잡한 수식 없이 `optimizer='adam'`과 같은 한 줄의 코드로 원하는 옵티마이저를 손쉽게 적용할 수 있습니다. 여기서는 그 내부 원리를 이해하는 데 초점을 맞춥니다.

---

## 1. 모멘텀 (Momentum)

### 정의

**모멘텀**은 경사 하강법에 **'관성'** 의 개념을 도입한 최적화 방식입니다. 이전 업데이트의 방향과 속도를 기억하여 현재 가중치를 업데이트할 때 반영함으로써, 학습 과정의 진동을 줄이고 수렴 속도를 높입니다.

### 비유와 설명

> **전략 비유**: 내리막길을 내려가는 무거운 공 🎳
>
> 공은 이전에 굴러온 방향과 속도(관성)가 있어, 작은 언덕이나 파인 홈(지역 최솟값)은 쉽게 통과하고, 일관된 내리막길에서는 점점 더 빠르게 가속합니다.

### 수식

$$
x_t = x_{t-1} - v_t
$$

$$
v_t = \gamma v_{t-1} + \eta \nabla f(x_{t-1})
$$

- **$x_t$**: 현재 스텝($t$)에서 업데이트될 **새로운 가중치**
- **$x_{t-1}$**: **이전 스텝**의 가중치
- **$v_t$**: 현재 스텝의 **이동 벡터**. 관성이 적용된 방향과 크기를 나타냅니다.
- **$\gamma$ (감마)**: **관성 계수**. 이전 이동 벡터($v_{t-1}$)를 얼마나 반영할지 결정하는 값으로, 보통 0.9와 같은 값을 사용합니다.
- **$\eta$ (에타)**: **학습률(Learning Rate)**
- **$\nabla f(x_{t-1})$**: 이전 가중치에 대한 **손실 함수의 기울기(Gradient)**

---

## 2. Adagrad (Adaptive Gradient)

### 정의

**Adagrad**는 각 가중치마다 **상황에 맞는 학습률을 자동으로 조절**하는 '적응형' 최적화 방식입니다. 변화가 많았던 가중치는 학습률을 줄이고, 변화가 적었던 가중치는 학습률을 크게 하여 학습을 진행합니다.

### 비유와 설명

> **전략 비유**: 신발 밑창이 닳는 등반가 👟
>
> 자주 밟는(변화가 많은) 길의 신발 밑창(학습률)은 빨리 닳아 보폭이 작아지고, 거의 밟지 않는(변화가 적은) 길의 밑창은 그대로라 보폭이 크게 유지됩니다.

### 수식

$$
x_t = x_{t-1} - \frac{\eta}{\sqrt{g_t + \epsilon}} \cdot \nabla f(x_{t-1})
$$

$$
g_t = g_{t-1} + (\nabla f(x_{t-1}))^2
$$

- **$g_t$**: **기울기 누적 값**. 이전 스텝까지의 모든 기울기 제곱 값을 **계속해서 더해 누적**한 값입니다.
- **$\epsilon$ (엡실론)**: 분모가 0이 되는 것을 방지하기 위한 아주 작은 값입니다. (e.g., `1e-8`)

### 한계

학습이 계속될수록 $g_t$가 무한정 커져서, 결국 학습률이 0에 가까워져 학습이 멈추는 문제가 발생할 수 있습니다.

---

## 3. RMSProp

### 정의

**RMSProp**은 Adagrad의 학습이 멈추는 문제를 해결하기 위해, 과거의 모든 기울기를 누적하는 대신 **최근 기울기 정보에 더 큰 가중치**를 두어 학습률을 조절하는 방식입니다.

### 비유와 설명

> **전략 비유**: 밑창이 스스로 복원되는 마법의 신발 ✨
>
> 신발 밑창(학습률)이 닳더라도, 최근에 밟지 않으면 조금씩 복원됩니다. 즉, 과거의 모든 변화가 아닌 **최근의 변화**에 더 집중합니다.

### 수식

$$
x_t = x_{t-1} - \frac{\eta}{\sqrt{g_t + \epsilon}} \cdot \nabla f(x_{t-1})
$$

$$
g_t = \gamma g_{t-1} + (1-\gamma)(\nabla f(x_{t-1}))^2
$$

- **$g_t$**: **기울기 제곱의 지수 이동 평균**. Adagrad와 달리 과거의 기울기는 점차 잊고, 최신 기울기 정보에 더 큰 비중을 둡니다.
- **$\gamma$ (감마)**: **지수 이동 평균의 감쇠율(decay rate)**. 과거 정보를 얼마나 유지할지 결정합니다.

---

## 4. Adam (Adaptive Moment Estimation)

### 정의

**Adam**은 **모멘텀**과 **RMSProp**의 핵심 아이디어를 결합한 최적화 방식으로, 현재 가장 널리 사용되는 표준 옵티마이저 중 하나입니다.

### 비유와 설명

> **전략 비유**: 관성으로 가속하는 무거운 공 + 마법의 신발 🚀
>
> **이동 방향은 부드럽게(모멘텀)**, **보폭(학습률)은 상황에 맞게 조절(RMSProp)** 하는 최고의 전략가입니다.

### 수식

Adam은 두 개의 '기억'을 관리합니다.

$$
v_t = \beta_1 v_{t-1} + (1-\beta_1)\nabla f(x_{t-1})
$$

$$
g_t = \beta_2 g_{t-1} + (1-\beta_2)(\nabla f(x_{t-1}))^2
$$

- **$v_t$**: 기울기의 **1차 모멘텀(방향성 기억)**. 모멘텀과 유사합니다.
- **$g_t$**: 기울기 제곱의 **2차 모멘텀(변화량 기억)**. RMSProp과 유사합니다.
- **$\beta_1, \beta_2$**: 각각 1차, 2차 모멘텀의 **지수 이동 평균 감쇠율**입니다.

학습 초반에 $v_t$와 $g_t$가 0에 가깝게 편향되는 것을 막기 위해 다음과 같은 **편향 보정(bias-correction)** 단계를 추가합니다.

$$
\hat{v}_t = \frac{v_t}{1 - \beta_1^t}, \quad \hat{g}_t = \frac{g_t}{1 - \beta_2^t}
$$

- **$\hat{v}_t, \hat{g}_t$**: **편향이 보정된** 1차, 2차 모멘텀 값.

최종 업데이트는 이 보정된 값들을 사용합니다. (최종 업데이트 수식은 RMSProp과 유사한 형태)

---

## ✨ 핵심 요약

다양한 옵티마이저들은 저마다의 전략으로 손실 함수의 최저점을 향해 나아갑니다.

| 옵티마이저  | 핵심 전략                        | 비유                      |
| :---------- | :------------------------------- | :------------------------ |
| **모멘텀**  | 이전의 이동 방향을 기억 (관성)   | 무거운 공                 |
| **Adagrad** | 변화가 많을수록 보폭 감소        | 닳는 신발                 |
| **RMSProp** | 최근 변화에 집중하여 보폭 조절   | 복원되는 신발             |
| **Adam**    | **방향(모멘텀) + 보폭(RMSProp)** | **관성이 있는 마법 신발** |

어떤 옵티마이저가 모든 문제에 대한 정답은 아니지만, 특별한 이유가 없다면 **Adam**을 기본으로 시작하는 것이 현대 딥러닝에서 가장 안정적이고 효과적인 선택으로 알려져 있습니다.
