# ⚙️ 신경망의 엔진: 순전파와 역전파의 수학적 원리

안녕하세요! 이번 시간에는 딥러닝의 핵심 엔진이라고 할 수 있는 **순전파(Forward Propagation)** 와 **역전파(Backpropagation)** 과정을 수학적으로 조금 더 깊이 있게 살펴보겠습니다. 신경망이 데이터를 어떻게 처리하고, 또 어떻게 스스로 똑똑해지는지 그 원리를 함께 파헤쳐 봅시다.

> **잠깐!** ✋
> 여기에 나오는 모든 수식을 100% 이해하지 못해도 괜찮습니다. 나중에 TensorFlow나 PyTorch 같은 프레임워크를 사용하면 이 모든 복잡한 계산은 자동으로 처리됩니다. 지금은 '아, 이런 원리로 신경망이 학습하는구나!' 하고 가볍게 흐름을 따라오시는 것만으로도 충분합니다.

---

## 학습에 필요한 두 가지 수학 도구

역전파의 원리를 이해하려면 미적분학의 두 가지 개념을 알아두면 좋습니다. 바로 **편미분**과 **연쇄 법칙(Chain Rule)** 입니다.

### 1. 편미분 (Partial Derivative)

**편미분**은 여러 변수를 가진 함수에서, **'오직 하나의 변수에만 집중'** 하여 그 변화가 함수 전체에 미치는 영향을 살펴보는 방법입니다. 나머지 변수들은 잠시 상수(고정된 값)처럼 취급하는 것이죠.

예를 들어, 함수 $f(x,y) = x^2y + 3xy + y^2$ 가 있을 때:

- $x$에 대한 편미분 ($\frac{\partial f}{\partial x}$) : $y$를 상수 취급 → $\frac{\partial f}{\partial x} = 2xy + 3y$
- $y$에 대한 편미분 ($\frac{\partial f}{\partial y}$) : $x$를 상수 취급 → $\frac{\partial f}{\partial y} = x^2 + 3x + 2y$

### 2. 연쇄 법칙 (Chain Rule)

**연쇄 법칙**은 함수 안에 또 다른 함수가 있는 **합성 함수**를 미분할 때 사용됩니다. 마치 양파 껍질을 벗기듯, 바깥 함수를 미분한 값에 안쪽 함수를 미분한 값을 연쇄적으로 곱해주는 방식입니다.

예를 들어, $y = f(g(x))$ 일 때 $y$를 $x$로 미분하면 다음과 같습니다.

$$\frac{dy}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$$

신경망은 여러 개의 층이 겹쳐진 거대한 합성 함수와 같습니다. 각 층의 연산이 이전 층의 결과를 입력으로 받기 때문이죠. 따라서 신경망의 학습 원리를 이해하려면 이 연쇄 법칙이 핵심적인 역할을 합니다.

---

## 간단한 신경망으로 계산 과정 살펴보기

이제 간단한 **2층 신경망(입력층-은닉층-출력층)** 을 예로 들어 순전파와 역전파의 계산 과정을 단계별로 따라가 보겠습니다.

- **입력 데이터**: $x$
- **가중치**: $w_1$, $w_2$
- **활성화 함수**: 시그모이드($\sigma$)

### 1단계: 순전파 (Forward Propagation) - 예측값 구하기

입력 데이터 $x$가 신경망을 통과하여 최종 예측값 $y_{pred}$를 만들어내는 과정입니다.

1.  **은닉층 계산**: 입력 $x$에 가중치 $w_1$을 곱하고($z_1$), 시그모이드 함수를 적용하여 중간 출력값 $a_1$을 만듭니다.

    $$z_1 = w_1 \cdot x$$

    $$a_1 = \sigma(z_1)$$

2.  **출력층 계산**: 은닉층의 출력 $a_1$에 가중치 $w_2$를 곱하고($z_2$), 다시 시그모이드 함수를 적용하여 최종 예측값 $y_{pred}$를 만듭니다.

    $$z_2 = w_2 \cdot a_1$$

    $$y_{pred} = \sigma(z_2)$$

### 2단계: 손실 계산 (Loss Calculation) - 오차 측정하기

모델이 얼마나 틀렸는지 측정할 차례입니다. 손실 함수로는 **평균 제곱 오차(Mean Squared Error, MSE)** 를 사용하겠습니다. ($y_{true}$는 실제 정답입니다)

$$L = \frac{1}{2}(y_{pred} - y_{true})^2$$

신경망의 목표는 이 손실($L$)을 최소화하는 것입니다. 그러기 위해 이제부터 역전파를 통해 각 가중치가 이 손실에 얼마나 영향을 미쳤는지 계산해 보겠습니다.

### 3단계: 역전파 (Backpropagation) - 오차의 원인 찾아가기

역전파는 순전파의 반대 방향으로, 연쇄 법칙을 이용해 각 가중치($w_1, w_2$)에 대한 손실의 기울기(gradient)를 계산하는 과정입니다.

#### **가중치 $w_2$에 대한 기울기 계산**

손실($L$)이 가중치($w_2$)에 얼마나 영향을 받는지 알려면, 연쇄 법칙에 따라 관련된 값들의 미분을 차례대로 곱해주면 됩니다.

$$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_2}$$

- $\frac{\partial L}{\partial y_{pred}}$ (예측값에 대한 손실의 변화량): $(y_{pred} - y_{true})$
- $\frac{\partial y_{pred}}{\partial z_2}$ (가중합에 대한 활성화 함수의 변화량): $\sigma'(z_2)$
- $\frac{\partial z_2}{\partial w_2}$ ($w_2$에 대한 가중합의 변화량): $a_1$

따라서 이들을 모두 곱하면 $w_2$의 기울기는 다음과 같습니다.

$$\frac{\partial L}{\partial w_2} = (y_{pred} - y_{true}) \cdot \sigma'(z_2) \cdot a_1$$

#### **가중치 $w_1$에 대한 기울기 계산**

$w_1$은 더 안쪽에 있으므로, 연쇄 법칙의 사슬이 더 길어집니다.

$$\frac{\partial L}{\partial w_1} = \left( \frac{\partial L}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial z_2} \right) \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$$

- 앞부분 $\left( \dots \right)$은 위에서 계산한 값들을 재활용합니다.
- $\frac{\partial z_2}{\partial a_1}$ (중간 출력값에 대한 두 번째 가중합의 변화량): $w_2$
- $\frac{\partial a_1}{\partial z_1}$ (첫 번째 가중합에 대한 활성화 함수의 변화량): $\sigma'(z_1)$
- $\frac{\partial z_1}{\partial w_1}$ ($w_1$에 대한 첫 번째 가중합의 변화량): $x$

이처럼 역전파는 출력층에서 계산된 기울기 값을 앞 층으로 계속 전달하며 효율적으로 계산을 수행합니다.

### 4단계: 가중치 업데이트 (Weight Update) - 모델 학습하기

이제 계산된 기울기를 사용하여 경사 하강법으로 가중치를 업데이트합니다. ($\eta$는 학습률입니다.)

$$w = w - \eta \cdot \frac{\partial L}{\partial w}$$

계산된 기울기 값을 위 식에 적용하면 다음과 같이 각 가중치가 업데이트됩니다.

$$w_2 = w_2 - \eta \cdot \frac{\partial L}{\partial w_2}$$
$$w_1 = w_1 - \eta \cdot \frac{\partial L}{\partial w_1}$$

이 과정을 수많은 데이터에 대해 반복하면, 가중치 $w_1$과 $w_2$는 점차 손실을 최소화하는 값으로 수렴하게 됩니다. 이것이 바로 신경망의 학습 원리입니다.

---

## 핵심 요약

> **신경망은 순전파를 통해 예측값을 만들고, 손실 함수로 오차를 계산합니다. 그 후, 역전파와 연쇄 법칙을 이용해 각 가중치에 대한 오차의 기울기를 계산하고, 경사 하강법으로 이 기울기를 빼주어 가중치를 업데이트하는 과정을 반복하며 학습합니다.**

- **순전파**: 입력 → 예측값 계산
- **손실 계산**: 예측값과 정답 비교
- **역전파**: 오차의 원인을 거슬러 올라가며 각 가중치의 책임(기울기) 계산
- **가중치 업데이트**: 계산된 기울기를 바탕으로 모델 수정

이 네 가지 단계의 순환이 바로 딥러닝 모델을 움직이는 강력한 엔진입니다.
